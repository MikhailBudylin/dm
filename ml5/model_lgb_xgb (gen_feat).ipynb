{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, cross_val_score as cv_scoring\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pylab as plt\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', delimiter=';')\n",
    "test  = pd.read_csv('test.csv', delimiter=';')\n",
    "\n",
    "test.replace({'None' : 999}, inplace=True)\n",
    "test[['smoke', 'alco', 'active']] = test[['smoke', 'alco', 'active']].astype(int)\n",
    "\n",
    "test.replace({999 : np.nan}, inplace=True)\n",
    "test[['gluc', 'cholesterol']] = test[['gluc', 'cholesterol']].astype(int)\n",
    "\n",
    "N_train = train.shape[0]\n",
    "data = pd.concat([train, test], 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean = False\n",
    "\n",
    "if clean:\n",
    "    bad_index = (data.height < 110) & (data.weight > 150)\n",
    "    buf = data.loc[bad_index, 'height']\n",
    "    data.loc[bad_index, 'height'] = data.loc[bad_index, 'weight']\n",
    "    data.loc[bad_index, 'weight'] = buf\n",
    "    \n",
    "    data.loc[data.height < 90, 'height'] += 100\n",
    "    \n",
    "    data.loc[(data.height >= 100) & (data.height < 110), 'height'] += 60 \n",
    "    \n",
    "    data.loc[(data.height >= 110) & (data.height < 120) & (data.weight < 110), 'height'] += 60\n",
    "    \n",
    "    data.loc[(data.height == data.ap_hi) & (data.weight == data.ap_lo), 'height'] = np.nan\n",
    "    data.loc[(data.height == data.ap_hi) & (data.weight == data.ap_lo), 'weight'] = np.nan\n",
    "    \n",
    "    data.loc[data.height <= 130, ['weight', 'height']] = [np.nan, np.nan]\n",
    "    data.loc[(data.height <= 135) & (data.weight > 60), 'height'] = np.nan\n",
    "    data.loc[(data.height < 140) & (data.weight > 80), 'height'] = np.nan\n",
    "    data.loc[data.height > 210, 'height'] = np.nan\n",
    "    \n",
    "    data.loc[data.weight < 20, 'weight'] *= 10\n",
    "    \n",
    "    data.loc[data.weight <= 30, 'weight'] = np.nan\n",
    "    data.loc[(data.weight < 36) & (data.height > 160), 'weight'] = np.nan\n",
    "    \n",
    "    data.loc[(data.weight > 140) & (data.height == data.weight), 'weight'] = np.nan\n",
    "    \n",
    "    data.loc[data.weight > 125, 'weight'] = np.nan\n",
    "    \n",
    "    data.replace({'weight': {120.0: np.nan}}, inplace=True)\n",
    "    \n",
    "    data['ap_hi'] = np.abs(data['ap_hi'])\n",
    "    data['ap_lo'] = np.abs(data['ap_lo'])\n",
    "\n",
    "    data.loc[data.ap_lo > 5000, 'ap_lo'] /= 100\n",
    "    data.loc[data.ap_hi > 5000, 'ap_hi'] /= 100\n",
    "    data.loc[data.ap_hi > 250, 'ap_hi'] /= 10\n",
    "    data.loc[data.ap_lo > 250, 'ap_lo'] /= 10\n",
    "    \n",
    "    data.loc[[12494, 60477, 51749], ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[[75399], 'ap_lo'] = 200\n",
    "    data.loc[6580, ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[data.ap_hi < 10, 'ap_hi'] = np.nan\n",
    "    \n",
    "    data.loc[data.ap_hi == 906, ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[data.ap_hi == 90.6, ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[data.ap_hi == 701, ['ap_hi', 'ap_lo']] = [110, 70]\n",
    "    data.loc[data.ap_hi == 309, ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[data.ap_hi == 30.9, ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[data.ap_hi == 806, ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[data.ap_hi == 509, ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[data.ap_hi == 50.9, ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[data.ap_hi == 50.9, ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[data.ap_hi == 40.1, ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    \n",
    "    data.loc[data.ap_hi == 400, 'ap_hi'] = 100\n",
    "    data.loc[data.ap_hi == 401, 'ap_hi'] = 101\n",
    "    data.loc[data.ap_hi == 410, 'ap_hi'] = 110\n",
    "    data.loc[data.ap_hi == 470, 'ap_hi'] = 170\n",
    "    data.loc[data.ap_lo == 410, 'ap_lo'] = 110\n",
    "    data.loc[data.ap_lo == 470, 'ap_lo'] = 170\n",
    "        \n",
    "    data.loc[(data.ap_hi == 138) & (data.ap_lo == 0), ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[(data.ap_hi == 149) & (data.ap_lo == 0), ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[(data.ap_hi == 90.7) & (data.ap_lo == 0), ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[(data.ap_hi == 148) & (data.ap_lo == 0), ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[(data.ap_hi == 80.6) & (data.ap_lo == 0), ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[(data.ap_hi == 108) & (data.ap_lo == 0), ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[(data.ap_hi == 121) & (data.ap_lo == 0), ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[(data.ap_hi == 117) & (data.ap_lo == 0), ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "    data.loc[(data.ap_hi == 118) & (data.ap_lo == 0), ['ap_hi', 'ap_lo']] = [np.nan, np.nan]\n",
    "\n",
    "    data.loc[data.ap_lo == 0, 'ap_lo'] = np.nan\n",
    "\n",
    "    data.replace({'ap_lo': {1: 100}}, inplace=True)\n",
    "\n",
    "    data.loc[data.ap_hi < 30, 'ap_hi'] *= 10\n",
    "    data.loc[data.ap_lo < 20, 'ap_lo'] *= 10\n",
    "    \n",
    "    data.replace({'ap_lo': {20: np.nan, 30: 80}}, inplace=True)\n",
    "    \n",
    "    bad_index = data.ap_hi < data.ap_lo\n",
    "    buf = data.loc[bad_index, 'ap_hi']\n",
    "    data.loc[bad_index, 'ap_hi'] = data.loc[bad_index, 'ap_lo']\n",
    "    data.loc[bad_index, 'ap_lo'] = buf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['chol+gluc'] = data['cholesterol'].astype(str) + data['gluc'].astype(str) \n",
    "data['chol+gluc'] = pd.factorize(data['chol+gluc'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['ap+chol'] = data['cholesterol'].astype(str) + data['ap_hi'].dropna().astype(int).astype(str)\n",
    "data['ap+chol'] = pd.factorize(data['ap+chol'])[0]\n",
    "\n",
    "data['age+ap'] = (data['age']//365).astype(str) + data['ap_hi'].astype(str)\n",
    "data['age+ap'] = pd.factorize(data['age+ap'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['imt'] = data['weight'] / (data['height']/100) ** 2\n",
    "\n",
    "data['ap-'] = data['ap_hi'] - data['ap_lo']\n",
    "data['ap*'] = data['ap_hi'] * data['ap_lo'] / 10000\n",
    "data['ap+'] = data['ap_hi'] + data['ap_lo']\n",
    "data['ap/'] = data['ap_hi'] / data['ap_lo']\n",
    "\n",
    "data['h/w'] = data['height'] / data['weight']\n",
    "data['h*w'] = data['height'] * data['weight'] / 10000\n",
    "\n",
    "data['age/ap'] = data['age'] / data['ap_hi']\n",
    "data['ap/age'] = data['ap_hi'] / data['age']\n",
    "data['age/w'] = data['age'] / data['weight']\n",
    "data['age/h'] = data['age'] / data['height']\n",
    "\n",
    "data['age*ap'] = data['age'] * data['ap_hi'] / 1000\n",
    "data['age*w']  = data['age'] * data['weight'] / 1000\n",
    "data['age*chol'] = data['age'] * data['cholesterol']\n",
    "data['age*gluc'] = data['age'] * data['gluc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_y = data.groupby(['cholesterol'])['cardio'].mean().reset_index()\n",
    "mean_y = mean_y.rename(columns = {'cardio' : 'p_chol'})\n",
    "data = pd.merge(data, mean_y, on = 'cholesterol', how = 'left')\n",
    "\n",
    "mean_y = data.groupby(['gluc'])['cardio'].mean().reset_index()\n",
    "mean_y = mean_y.rename(columns = {'cardio' : 'p_gluc'})\n",
    "data = pd.merge(data, mean_y, on = 'gluc', how = 'left')\n",
    "\n",
    "mean_y = data.groupby(['chol+gluc'])['cardio'].mean().reset_index()\n",
    "mean_y = mean_y.rename(columns = {'cardio' : 'p_ch+gl'})\n",
    "data = pd.merge(data, mean_y, on = 'chol+gluc', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected_Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_feat = [u'active', u'age', u'alco', u'ap_hi', u'ap_lo', u'gender', u'height',\n",
    "       u'smoke', u'chol+gluc', u'ap+chol', u'imt', u'ap*', u'ap+', u'h/w',\n",
    "       u'h*w', u'ap/age', u'age*ap', u'age*w', u'age*gluc', u'p_chol',\n",
    "       u'p_gluc', u'p_ch+gl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = data[:N_train].drop(['id', 'cardio'], 1)\n",
    "y_train = data[:N_train]['cardio']\n",
    "\n",
    "x_test = data[N_train:].drop(['id', 'cardio'], 1)\n",
    "\n",
    "#x_train = data.loc[:int(0.8*N_train)-1].drop(['id', 'cardio'], 1)\n",
    "#y_train = data.loc[:int(0.8*N_train)-1]['cardio']\n",
    "\n",
    "#x_test = data.loc[int(0.8*N_train):N_train-1].drop(['id', 'cardio'], 1)\n",
    "#y_test = data.loc[int(0.8*N_train):N_train-1]['cardio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV_and_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective' : 'binary',\n",
    "    'metric' : 'binary_logloss',\n",
    "    'num_leaves' : 16,\n",
    "    'max_depth' : 5,\n",
    "    'learning_rate' : 0.03,\n",
    "    'feature_fraction' : 0.9,\n",
    "    'bagging_fraction' : 0.9,\n",
    "    'bagging_freq' : 3,\n",
    "    'max_bin' : 500,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ltrain = lgb.Dataset(x_train[use_feat], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result = lgb.cv(lgb_params, ltrain, num_boost_round=1000, nfold=5, \n",
    "                   early_stopping_rounds=20, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(x_train[use_feat], y_train)\n",
    "dtest  = xgb.DMatrix(x_test[use_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'objective' : 'binary:logistic',\n",
    "    'eval_metric' : 'logloss',\n",
    "    'max_depth' : 5,\n",
    "    'eta' : 0.02,\n",
    "    'subsample' : 0.9,\n",
    "    'colsample_bytree' : 0.9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result = xgb.cv(xgb_params, dtrain, num_boost_round=1000, nfold=5, \n",
    "                   early_stopping_rounds=20, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "\n",
    "def get_score(x_train, y_train, n_splits):\n",
    "    kfold = KFold(n_splits=n_splits)\n",
    "    y_oof_lgb  = np.empty(len(x_train))\n",
    "\n",
    "    for train_idx, test_idx in kfold.split(x_train):\n",
    "        \n",
    "        ltrain_cv = lgb.Dataset(x_train.loc[train_idx], y_train[train_idx])\n",
    "        ltest_cv  = lgb.Dataset(x_train.loc[test_idx], y_train[test_idx])\n",
    "        eval_list = ltest_cv\n",
    "        \n",
    "        lgbm = lgb.train(lgb_params, ltrain_cv, num_boost_round = 1000, \n",
    "                          valid_sets=eval_list, early_stopping_rounds = 20, verbose_eval = 0)\n",
    "        \n",
    "        y_oof_lgb[test_idx] = lgbm.predict(x_train.loc[test_idx])\n",
    "    return log_loss(y_train, y_oof_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_alg_start = False\n",
    "\n",
    "if gen_alg_start:\n",
    "    # number of epochs\n",
    "    epochs = 20\n",
    "    #initial propability\n",
    "    p0 = 0.8\n",
    "    #crossingover probability\n",
    "    p_cross = 0.5\n",
    "    #mutate probability\n",
    "    p_mut = 0.015\n",
    "    #number of the best samples\n",
    "    N_best = 30\n",
    "    N_features = x_train.shape[1]\n",
    "    #initialization \n",
    "    samples = np.random.binomial(1, p0, ( N_best, N_features)).astype(bool)\n",
    "    \n",
    "    all_features = x_train.columns\n",
    "    scores = np.array([])\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for i in range(N_best):\n",
    "            j = np.random.randint(0, N_best)\n",
    "            cross = np.random.binomial(1, p_cross, N_features).astype(bool)\n",
    "            if (samples[i][cross] != samples[j][cross]).any():\n",
    "                new_sample = np.copy(samples[i])\n",
    "                new_sample[cross] = np.copy(samples[j][cross])\n",
    "                if new_sample.any():\n",
    "                    samples = np.vstack([samples, new_sample])\n",
    "        for i in range(N_best, len(samples)):\n",
    "            mutate = np.random.binomial(1, p_mut, N_features).astype(bool)\n",
    "            samples[i][mutate] = ~samples[i][mutate]\n",
    "        for i in range(int(N_best*0.2)):\n",
    "            new_sample = np.random.binomial(1, p0, N_features).astype(bool)\n",
    "            if new_sample.any():\n",
    "                samples = np.vstack([samples, new_sample])\n",
    "        for i, sample in enumerate(samples[N_best:]):\n",
    "            \n",
    "            score = get_score(x_train[all_features[sample]], y_train, 5)\n",
    "            \n",
    "            print score\n",
    "            \n",
    "            scores = np.append(scores, score)\n",
    "        ind_best = scores.argsort()\n",
    "        print 'epoch = ', epoch+1\n",
    "        print scores[ind_best][:5]\n",
    "        scores = scores[ind_best][:N_best]\n",
    "        samples = samples[ind_best]\n",
    "        samples = samples[:N_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features[samples[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "\n",
    "y_pred_lgb = np.zeros(len(x_test))\n",
    "y_oof_lgb  = np.empty(len(x_train))\n",
    "\n",
    "for train_idx, test_idx in kfold.split(x_train):\n",
    "    ltrain_cv = lgb.Dataset(x_train[use_feat].loc[train_idx], y_train[train_idx])\n",
    "    ltest_cv  = lgb.Dataset(x_train[use_feat].loc[test_idx], y_train[test_idx])\n",
    "    eval_list = ltest_cv\n",
    "    \n",
    "    lgbm = lgb.train(lgb_params, ltrain_cv, num_boost_round = 1000, \n",
    "                      valid_sets=eval_list, early_stopping_rounds = 20, verbose_eval = 50)\n",
    "    \n",
    "    y_oof_lgb[test_idx] = lgbm.predict(x_train[use_feat].loc[test_idx])\n",
    "    \n",
    "    y_pred_lgb += lgbm.predict(x_test[use_feat])\n",
    "y_pred_lgb /= n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "\n",
    "y_pred_xgb = np.zeros(len(x_test))\n",
    "y_oof_xgb  = np.empty(len(x_train))\n",
    "dtest  = xgb.DMatrix(x_test[use_feat])\n",
    "\n",
    "for train_idx, test_idx in kfold.split(x_train):\n",
    "    dtrain_cv = xgb.DMatrix(x_train[use_feat].loc[train_idx], y_train[train_idx])\n",
    "    dtest_cv  = xgb.DMatrix(x_train[use_feat].loc[test_idx], y_train[test_idx])\n",
    "    eval_list = [(dtest_cv, 'test')]\n",
    "    \n",
    "    xgbst = xgb.train(xgb_params, dtrain_cv, num_boost_round = 1000, \n",
    "                      evals=eval_list, early_stopping_rounds = 20, verbose_eval = 50)\n",
    "    \n",
    "    y_oof_xgb[test_idx] = xgbst.predict(dtest_cv)\n",
    "    \n",
    "    y_pred_xgb += xgbst.predict(dtest)\n",
    "y_pred_xgb /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print log_loss(y_train, y_oof_lgb*0.5 + y_oof_xgb*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_res = (y_pred_lgb + y_pred_xgb)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "ax = plt.subplot(111)\n",
    "lgb.plot_importance(lgbm, ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Write_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(y_res)\n",
    "output.to_csv('lgb+xgb_new_feat.csv', header=None, index=None)\n",
    "output.head(7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
